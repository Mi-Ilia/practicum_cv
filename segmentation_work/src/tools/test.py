import os
import sys
import argparse
from pathlib import Path
import warnings
import logging
import json
from datetime import datetime
from typing import List, Optional, Dict, Any
import time
import numpy as np
import torch
from PIL import Image
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False

# –û—Ç–∫–ª—é—á–∞–µ–º –±—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≤ Jupyter
if 'PYTHONUNBUFFERED' not in os.environ:
    os.environ['PYTHONUNBUFFERED'] = '1'

warnings.filterwarnings('ignore', category=UserWarning)
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
mmseg_path = os.path.join(project_root, 'mmsegmentation')
if os.path.exists(mmseg_path) and mmseg_path not in sys.path:
    sys.path.insert(0, mmseg_path)
sys.path.append(project_root)

from mmengine.config import Config, DictAction
from mmengine.runner import Runner
from mmengine.hooks import Hook
from mmseg.utils import register_all_modules
from src.utils.test_utils import setup_test_config, get_class_names, get_class_names_from_dataloader
from src.utils.visualization_utils import get_palette, visualize_prediction

# –ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
_collected_results = []

class PerImageMetricsHook(Hook):
    """–•—É–∫ –¥–ª—è —Å–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ–∫–∞—Ä—Ç–∏–Ω–Ω–æ –≤–æ –≤—Ä–µ–º—è test().
    
    –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏:
    1. –°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ GT
    2. –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π IoUMetric (compute_metrics_per_image)
    3. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    
    –í –∫–æ–Ω—Ü–µ –º–æ–∂–Ω–æ —É—Å—Ä–µ–¥–Ω–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –≤—Å–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∞–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫.
    """
    
    def __init__(self, num_classes: int = 3):
        super().__init__()
        self.results = []
        self.metrics = []
        self.num_classes = num_classes
        self._first_batch_logged = False
    
    def after_test_iter(self, runner, batch_idx: int, data_batch: dict = None, outputs: list = None):
        """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏."""
        if outputs is None or not outputs:
            return
        
        # –ü–æ–ª—É—á–∞–µ–º data_samples –∏–∑ data_batch
        data_samples = data_batch.get('data_samples', []) if data_batch else []
        
        for idx, output in enumerate(outputs):
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                if not hasattr(output, 'pred_sem_seg') or output.pred_sem_seg is None:
                    continue
                
                pred_data = output.pred_sem_seg.data
                if isinstance(pred_data, torch.Tensor):
                    pred_sem_seg = pred_data.cpu().numpy()
                else:
                    pred_sem_seg = pred_data
                if pred_sem_seg.ndim == 3 and pred_sem_seg.shape[0] == 1:
                    pred_sem_seg = pred_sem_seg[0]
                
                # –ü–æ–ª—É—á–∞–µ–º GT (–∏–∑ output –∏–ª–∏ –∏–∑ data_samples)
                gt_sem_seg = None
                if hasattr(output, 'gt_sem_seg') and output.gt_sem_seg is not None:
                    gt_data = output.gt_sem_seg.data
                    if isinstance(gt_data, torch.Tensor):
                        gt_sem_seg = gt_data.cpu().numpy()
                    else:
                        gt_sem_seg = gt_data
                    if gt_sem_seg.ndim == 3 and gt_sem_seg.shape[0] == 1:
                        gt_sem_seg = gt_sem_seg[0]
                elif idx < len(data_samples) and hasattr(data_samples[idx], 'gt_sem_seg'):
                    gt_data = data_samples[idx].gt_sem_seg.data
                    if isinstance(gt_data, torch.Tensor):
                        gt_sem_seg = gt_data.cpu().numpy()
                    else:
                        gt_sem_seg = gt_data
                    if gt_sem_seg.ndim == 3 and gt_sem_seg.shape[0] == 1:
                        gt_sem_seg = gt_sem_seg[0]
                
                if gt_sem_seg is None:
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π IoUMetric
                metrics_dict = compute_metrics_per_image(
                    pred_sem_seg, gt_sem_seg, self.num_classes,
                    ignore_index=255, iou_metrics=['mIoU', 'mDice']
                )
                
                # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
                img_path = None
                if hasattr(output, 'img_path'):
                    img_path = output.img_path
                elif hasattr(output, 'metainfo') and 'img_path' in output.metainfo:
                    img_path = output.metainfo['img_path']
                elif idx < len(data_samples) and hasattr(data_samples[idx], 'img_path'):
                    img_path = data_samples[idx].img_path
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result = {
                    'output': output,
                    'pred_sem_seg': pred_sem_seg,
                    'gt_sem_seg': gt_sem_seg,
                    'metrics': metrics_dict['main'],  # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    'per_class_metrics': metrics_dict['per_class'],  # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
                    'img_path': img_path,
                    'batch_idx': batch_idx,
                    'idx': idx
                }
                
                self.results.append(result)
                
            except Exception as e:
                logger.debug(f"Error processing image in batch {batch_idx}, idx {idx}: {e}")
                continue
    
    def after_test_epoch(self, runner, metrics=None):
        """–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""
        global _collected_results
        _collected_results = self.results.copy()
        if len(_collected_results) == 0:
            logger.error(f"‚ùå PerImageMetricsHook: Collected 0 results!")

def compute_metrics_per_image(pred_mask: np.ndarray, gt_mask: np.ndarray, 
                              num_classes: int, ignore_index: int = 255,
                              iou_metrics: List[str] = ['mIoU', 'mDice']) -> Dict[str, Any]:
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π IoUMetric.
    
    Args:
        pred_mask: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–∞—Å–∫–∞ (H, W) –∫–∞–∫ numpy array
        gt_mask: Ground truth –º–∞—Å–∫–∞ (H, W) –∫–∞–∫ numpy array
        num_classes: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
        ignore_index: –ò–Ω–¥–µ–∫—Å –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 255)
        iou_metrics: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è ['mIoU', 'mDice']
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏:
        - 'main': —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (mIoU, mDice, mAcc, aAcc)
        - 'per_class': –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º {class_idx: {'IoU': ..., 'Dice': ..., 'Acc': ...}}
    """
    from mmseg.evaluation.metrics.iou_metric import IoUMetric
    
    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π IoUMetric
    iou_metric = IoUMetric(
        ignore_index=ignore_index,
        iou_metrics=iou_metrics,
        format_only=False
    )
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º dataset_meta (–Ω—É–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç—ã)
    iou_metric.dataset_meta = {
        'classes': [f'class_{i}' for i in range(num_classes)],
        'label_map': {},
        'reduce_zero_label': False
    }
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –º–∞—Å–∫–∏ –≤ torch.Tensor
    pred_tensor = torch.from_numpy(pred_mask.astype(np.int64))
    gt_tensor = torch.from_numpy(gt_mask.astype(np.int64))
    
    # –í—ã—á–∏—Å–ª—è–µ–º intersect_and_union –¥–ª—è –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏
    area_intersect, area_union, area_pred_label, area_label = iou_metric.intersect_and_union(
        pred_tensor, gt_tensor, num_classes, ignore_index
    )
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ numpy (–±–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å —É—á–µ—Ç–æ–º —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤)
    def safe_to_numpy(x):
        if isinstance(x, torch.Tensor):
            return x.cpu().numpy()
        elif isinstance(x, np.ndarray):
            return x
        elif isinstance(x, (np.number, np.bool_)):
            # numpy —Å–∫–∞–ª—è—Ä—ã - –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∞—Å—Å–∏–≤
            return np.asarray(x)
        else:
            return np.array(x)
    
    area_intersect = safe_to_numpy(area_intersect)
    area_union = safe_to_numpy(area_union)
    area_pred_label = safe_to_numpy(area_pred_label)
    area_label = safe_to_numpy(area_label)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä—É—á–Ω—É—é, —Ç.–∫. total_area_to_metrics –∏–º–µ–µ—Ç –±–∞–≥ —Å numpy —Å–∫–∞–ª—è—Ä–∞–º–∏
    ret_metrics = {}
    
    # IoU
    with np.errstate(divide='ignore', invalid='ignore'):
        iou = area_intersect / area_union
        ret_metrics['IoU'] = iou
        
        # Acc (per-class accuracy)
        acc = area_intersect / area_label
        ret_metrics['Acc'] = acc
        
        # Dice
        dice = 2 * area_intersect / (area_pred_label + area_label)
        ret_metrics['Dice'] = dice
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º nan_to_num –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if iou_metric.nan_to_num is not None:
        ret_metrics = {
            metric: np.nan_to_num(metric_value, nan=iou_metric.nan_to_num)
            for metric, metric_value in ret_metrics.items()
        }
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    ret_metrics_summary = {
        ret_metric: np.round(np.nanmean(ret_metric_value) * 100, 2)
        for ret_metric, ret_metric_value in ret_metrics.items()
    }
    
    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    main_metrics = {}
    for key, val in ret_metrics_summary.items():
        if key == 'aAcc':
            main_metrics[key] = float(val)
        else:
            main_metrics['m' + key] = float(val)
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º
    per_class_metrics = {}
    if 'IoU' in ret_metrics:
        iou_per_class = ret_metrics['IoU']
        dice_per_class = ret_metrics.get('Dice', np.zeros_like(iou_per_class))
        acc_per_class = ret_metrics.get('Acc', np.zeros_like(iou_per_class))
        
        for class_idx in range(num_classes):
            per_class_metrics[class_idx] = {
                'IoU': float(np.round(iou_per_class[class_idx] * 100, 2)),
                'Dice': float(np.round(dice_per_class[class_idx] * 100, 2)),
                'Acc': float(np.round(acc_per_class[class_idx] * 100, 2))
            }
    
    return {
        'main': main_metrics,
        'per_class': per_class_metrics
    }

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —è–≤–Ω—ã–º –≤—ã–≤–æ–¥–æ–º –≤ stdout –¥–ª—è Jupyter
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º stdout
    ],
    force=True  # –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
)
logger = logging.getLogger('TestScript')
# –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—ã–≤–æ–¥ –Ω–µ –±—É—Ñ–µ—Ä–∏–∑—É–µ—Ç—Å—è
sys.stdout.reconfigure(line_buffering=True) if hasattr(sys.stdout, 'reconfigure') else None

def trigger_visualization_hook(cfg, args):
    default_hooks = cfg.default_hooks
    if 'visualization' in default_hooks:
        visualization_hook = default_hooks['visualization']
        # Turn on visualization
        visualization_hook['draw'] = True
        if args.show:
            visualization_hook['show'] = True
            visualization_hook['wait_time'] = args.wait_time
        if args.show_dir:
            visualizer = cfg.visualizer
            visualizer['save_dir'] = args.show_dir
    else:
        raise RuntimeError(
            'VisualizationHook must be included in default_hooks.'
            'refer to usage '
            '"visualization=dict(type=\'VisualizationHook\')"')

    return cfg

def save_metrics_to_json(results, output_dir, config_path, checkpoint_path, split='test', runner=None, cfg=None, 
                        inference_metrics=None, per_class_metrics_from_evaluator=None):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –≤ JSON —Ñ–∞–π–ª.
    
    ‚ö†Ô∏è –í–ê–ñ–ù–û: 
    - –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (mIoU, mDice, mAcc, aAcc) –±–µ—Ä—É—Ç—Å—è –∏–∑ results (evaluator.compute_metrics())
    - –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –±–µ—Ä—É—Ç—Å—è –∏–∑ per_class_metrics_from_evaluator (–∏–∑ evaluator.results)
    - –ü–æ–∫–∞—Ä—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ù–ï –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ (—Ç–æ–ª—å–∫–æ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏)
    """
    if output_dir is None:
        output_dir = os.path.join('./work_dirs', 'test_results')
    os.makedirs(output_dir, exist_ok=True)
    
    classes_info = get_class_names(cfg) if cfg is not None else None
    
    metrics_data = {
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'config': str(config_path),
            'checkpoint': str(checkpoint_path),
            'split': split,
            'classes': classes_info if classes_info else None
        },
        'metrics': {
            'main': {},
            'per_class': {},
            'inference': {}
        }
    }
    
    all_metrics = {}
    
    if results is not None and isinstance(results, dict):
        all_metrics.update(results)
    
    if runner is not None and hasattr(runner, 'message_hub'):
        try:
            for prefix in ['test', 'val']:
                scalars = runner.message_hub.get_scalar(prefix)
                if scalars:
                    for key, buf in scalars.items():
                        try:
                            all_metrics[key] = buf.current()
                        except Exception:
                            pass
        except Exception:
            pass
    
    # –ü–∞—Ä—Å–∏–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ results (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ evaluator.compute_metrics())
    # ‚ö†Ô∏è –í–ê–ñ–ù–û: –≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ (–≥–ª–æ–±–∞–ª—å–Ω—ã–µ), –Ω–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–µ –ø–æ–∫–∞—Ä—Ç–∏–Ω–Ω—ã–µ!
    per_class_by_idx = {}  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º
    
    for key, value in all_metrics.items():
        if not isinstance(value, (int, float)):
            continue
            
        value = float(value)
        clean_key = key.replace('test/', '').replace('val/', '')
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (mIoU, mDice, mAcc, aAcc, mFscore, etc.) - –ì–õ–û–ë–ê–õ–¨–ù–´–ï –∏–∑ evaluator
        if clean_key in ['mIoU', 'mDice', 'mAcc', 'aAcc', 'mFscore', 'mPrecision', 'mRecall']:
            metrics_data['metrics']['main'][clean_key] = value
        # –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ "IoU_class_0", "Dice_class_1", "Acc_class_2"
        # –∏–ª–∏ "IoU.class_0", "Dice.class_1" (—Å —Ç–æ—á–∫–æ–π)
        elif '_class_' in clean_key or '.class_' in clean_key:
            try:
                # –ü—Ä–æ–±—É–µ–º –æ–±–∞ —Ñ–æ—Ä–º–∞—Ç–∞: —Å —Ç–æ—á–∫–æ–π –∏ —Å –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ–º
                if '.class_' in clean_key:
                    parts = clean_key.split('.class_')
                else:
                    parts = clean_key.split('_class_')
                
                if len(parts) == 2:
                    metric_name = parts[0]  # IoU, Dice, Acc, etc.
                    class_idx = int(parts[1])
                    
                    if class_idx not in per_class_by_idx:
                        per_class_by_idx[class_idx] = {}
                    per_class_by_idx[class_idx][metric_name] = value
            except (ValueError, IndexError):
                metrics_data['metrics']['main'][clean_key] = value
        # –ú–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ –º–∞—Å—Å–∏–≤–æ–≤ (–µ—Å–ª–∏ –ø—Ä–∏—Ö–æ–¥—è—Ç –∫–∞–∫ —Å–ø–∏—Å–∫–∏)
        elif isinstance(value, (list, tuple)) and len(value) > 0:
            # –ï—Å–ª–∏ —ç—Ç–æ –º–∞—Å—Å–∏–≤ –º–µ—Ç—Ä–∏–∫ –ø–æ –∫–ª–∞—Å—Å–∞–º
            if clean_key in ['IoU', 'Dice', 'Acc'] and classes_info:
                for class_idx, metric_value in enumerate(value):
                    if class_idx not in per_class_by_idx:
                        per_class_by_idx[class_idx] = {}
                    per_class_by_idx[class_idx][clean_key] = float(metric_value)
        else:
            # –û—Å—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ main
            metrics_data['metrics']['main'][clean_key] = value
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏)
    # ‚ö†Ô∏è –í–ê–ñ–ù–û: –≠—Ç–∏ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø–ª–æ—â–∞–¥–µ–π –ø–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º
    if per_class_metrics_from_evaluator:
        for class_idx, class_metrics in per_class_metrics_from_evaluator.items():
            if class_idx not in per_class_by_idx:
                per_class_by_idx[class_idx] = {}
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —É –º–µ—Ç—Ä–∏–∫, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã—Ö –∏–∑ –ø–æ–∫–∞—Ä—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            per_class_by_idx[class_idx].update(class_metrics)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º per_class_by_idx –≤ —Ñ–æ—Ä–º–∞—Ç —Å –∏–º–µ–Ω–∞–º–∏ –∫–ª–∞—Å—Å–æ–≤
    if classes_info:
        for class_idx, class_metrics in per_class_by_idx.items():
            if class_idx < len(classes_info):
                class_name = classes_info[class_idx]
                metrics_data['metrics']['per_class'][class_name] = class_metrics
            else:
                # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤—ã—Ö–æ–¥–∏—Ç –∑–∞ –ø—Ä–µ–¥–µ–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å
                metrics_data['metrics']['per_class'][f'Class_{class_idx}'] = class_metrics
    else:
        # –ï—Å–ª–∏ –Ω–µ—Ç –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
        for class_idx, class_metrics in per_class_by_idx.items():
            metrics_data['metrics']['per_class'][f'Class_{class_idx}'] = class_metrics
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ inference (FPS, latency)
    if inference_metrics:
        metrics_data['metrics']['inference'].update(inference_metrics)
    elif runner is not None:
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–∑ message_hub
        try:
            if hasattr(runner, 'message_hub'):
                timer_info = runner.message_hub.get_scalar('train') or runner.message_hub.get_scalar('test')
                if timer_info:
                    for key in ['time', 'data_time', 'iter_time']:
                        if key in timer_info:
                            try:
                                time_value = timer_info[key].current()
                                if time_value > 0:
                                    fps = 1000.0 / time_value if 'iter_time' in key else None
                                    latency_ms = time_value
                                    if fps:
                                        metrics_data['metrics']['inference']['fps'] = round(fps, 2)
                                    metrics_data['metrics']['inference']['latency_ms'] = round(latency_ms, 2)
                                    break
                            except Exception:
                                pass
        except Exception:
            pass
    
    # –ï—Å–ª–∏ inference –º–µ—Ç—Ä–∏–∫–∏ –ø—É—Å—Ç—ã–µ, —É–¥–∞–ª—è–µ–º —Å–µ–∫—Ü–∏—é
    if not metrics_data['metrics']['inference']:
        metrics_data['metrics'].pop('inference', None)
    
    metrics_file = os.path.join(output_dir, 'test_metrics.json')
    try:
        with open(metrics_file, 'w', encoding='utf-8') as f:
            json.dump(metrics_data, f, indent=2, ensure_ascii=False)
        logger.info(f"üìä Metrics saved to: {metrics_file}")
        
        if metrics_data['metrics']['main']:
            logger.info("="*80)
            logger.info("üìà Test Metrics Summary:")
            logger.info("-"*80)
            for key, value in metrics_data['metrics']['main'].items():
                logger.info(f"  {key}: {value:.4f}")
            
            if metrics_data['metrics']['per_class']:
                logger.info("-"*80)
                logger.info("üìä Per-Class Metrics:")
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∞–º –∫–ª–∞—Å—Å–æ–≤ –∏–ª–∏ –∏–Ω–¥–µ–∫—Å–∞–º
                sorted_classes = sorted(metrics_data['metrics']['per_class'].keys())
                for class_name in sorted_classes:
                    class_metrics = metrics_data['metrics']['per_class'][class_name]
                    logger.info(f"  {class_name}:")
                    for metric_name, metric_value in class_metrics.items():
                        logger.info(f"    {metric_name}: {metric_value:.4f}")
            
            if 'inference' in metrics_data['metrics'] and metrics_data['metrics']['inference']:
                logger.info("-"*80)
                logger.info("‚ö° Inference Metrics:")
                for key, value in metrics_data['metrics']['inference'].items():
                    logger.info(f"  {key}: {value}")
            logger.info("="*80)
    except Exception as e:
        logger.error(f"Failed to save metrics: {e}")


def visualize_top_predictions(runner: Runner, top_n: int, split: str, output_dir: str, class_names: Optional[List[str]] = None):
    """–í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–æ–ø N –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞.
    
    ‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–æ–∫–∞—Ä—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¢–û–õ–¨–ö–û –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ top-N.
    –û–Ω–∏ –º–æ–≥—É—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –∏–∑ evaluator (–æ—Å–æ–±–µ–Ω–Ω–æ –ø—Ä–∏ –ø—É—Å—Ç—ã—Ö –º–∞—Å–∫–∞—Ö).
    
    Args:
        runner: Runner —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
        top_n: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        split: –†–∞–∑–¥–µ–ª –¥–∞—Ç–∞—Å–µ—Ç–∞ (train/val/test) - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
        output_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        class_names: –ò–º–µ–Ω–∞ –∫–ª–∞—Å—Å–æ–≤
    """
    global _collected_results
    
    logger.info("="*80)
    logger.info(f"üñºÔ∏è  Visualizing top {top_n} predictions...")
    
    if not _collected_results or len(_collected_results) == 0:
        logger.warning("‚ö†Ô∏è  No results collected from first pass. Visualization skipped.")
        return
    
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–ª–∞—Å—Å–∞—Ö
    model = runner.model
    num_classes = model.decode_head.num_classes if hasattr(model, 'decode_head') else 3
    if class_names is None:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º dataloader –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏–º–µ–Ω –∫–ª–∞—Å—Å–æ–≤
        if split == 'test':
            dataloader = runner.test_dataloader
        elif split == 'val':
            dataloader = runner.val_dataloader
        else:
            dataloader = runner.train_dataloader
        
        if dataloader is not None:
            class_names = get_class_names_from_dataloader(dataloader)
        
        if class_names is None:
            class_names = [f'Class {i}' for i in range(num_classes)]
    
    palette = get_palette(num_classes)
    all_results = []
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏–∑ —Ö—É–∫–∞ (—É–∂–µ —Å–æ–¥–µ—Ä–∂–∞—Ç pred_sem_seg, gt_sem_seg, metrics)
    for result_item in _collected_results:
        try:
            pred_sem_seg = result_item['pred_sem_seg'].copy()  # –ö–æ–ø–∏—Ä—É–µ–º —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª
            gt_sem_seg = result_item['gt_sem_seg'].copy()
            metrics = result_item['metrics']
            img_path = result_item.get('img_path')
            ori_shape = result_item.get('ori_shape')
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            if img_path:
                try:
                    img = np.array(Image.open(img_path).convert('RGB'))
                    h_img, w_img = img.shape[:2]
                    h_pred, w_pred = pred_sem_seg.shape[:2]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –∏ —Ä–µ—Å–∞–π–∑–∏–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
                    if (h_img != h_pred) or (w_img != w_pred):
                        target_h = ori_shape[0] if ori_shape is not None else h_img
                        target_w = ori_shape[1] if ori_shape is not None else w_img
                        
                        # –†–µ—Å–∞–π–∑–∏–º –º–∞—Å–∫–∏ –∫ —Ä–∞–∑–º–µ—Ä—É –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if CV2_AVAILABLE:
                            pred_sem_seg = cv2.resize(
                                pred_sem_seg.astype(np.uint8),
                                (target_w, target_h),
                                interpolation=cv2.INTER_NEAREST
                            ).astype(pred_sem_seg.dtype)
                            gt_sem_seg = cv2.resize(
                                gt_sem_seg.astype(np.uint8),
                                (target_w, target_h),
                                interpolation=cv2.INTER_NEAREST
                            ).astype(gt_sem_seg.dtype)
                        else:
                            from PIL import Image as PILImage
                            pred_sem_seg = np.array(
                                PILImage.fromarray(pred_sem_seg.astype(np.uint8))
                                .resize((target_w, target_h), PILImage.NEAREST)
                            ).astype(pred_sem_seg.dtype)
                            gt_sem_seg = np.array(
                                PILImage.fromarray(gt_sem_seg.astype(np.uint8))
                                .resize((target_w, target_h), PILImage.NEAREST)
                            ).astype(gt_sem_seg.dtype)
                        
                        # –†–µ—Å–∞–π–∑–∏–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∫ —Ü–µ–ª–µ–≤–æ–º—É —Ä–∞–∑–º–µ—Ä—É (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
                        if (h_img != target_h) or (w_img != target_w):
                            if CV2_AVAILABLE:
                                img = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_LINEAR)
                            else:
                                from PIL import Image as PILImage
                                img = np.array(
                                    PILImage.fromarray(img)
                                    .resize((target_w, target_h), PILImage.BILINEAR)
                                )
                except Exception as e:
                    logger.warning(f"Failed to load image {img_path}: {e}")
                    img = np.zeros((pred_sem_seg.shape[0], pred_sem_seg.shape[1], 3), dtype=np.uint8)
            else:
                img = np.zeros((pred_sem_seg.shape[0], pred_sem_seg.shape[1], 3), dtype=np.uint8)
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–º—è —Ñ–∞–π–ª–∞
            if img_path:
                img_name = Path(img_path).stem
            else:
                img_name = f'img_{result_item["batch_idx"]}_{result_item["idx"]}'
            
            all_results.append({
                'image': img,
                'pred_mask': pred_sem_seg,
                'gt_mask': gt_sem_seg,
                'metrics': metrics,
                'image_name': img_name,
                'img_path': img_path
            })
        except Exception as e:
            logger.debug(f"Error processing result item: {e}")
            continue
    
    
    if len(all_results) == 0:
        logger.warning("No results collected, skipping visualization")
        return
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ mDice (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º NaN - —Å—Ç–∞–≤–∏–º –∏—Ö –≤ –∫–æ–Ω–µ—Ü)
    def sort_key(x):
        dice = x['metrics']['mDice']
        # NaN –∏ None —Å—á–∏—Ç–∞–µ–º —Ö—É–¥—à–∏–º–∏ (—Å—Ç–∞–≤–∏–º –≤ –∫–æ–Ω–µ—Ü)
        if dice is None or (isinstance(dice, float) and np.isnan(dice)):
            return -float('inf')
        return dice
    
    all_results.sort(key=sort_key, reverse=True)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º top_n –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    actual_top_n = min(top_n, len(all_results))
    
    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø N –ª—É—á—à–∏—Ö –∏ —Ö—É–¥—à–∏—Ö
    top_correct = all_results[:actual_top_n]
    top_incorrect = all_results[-actual_top_n:] if len(all_results) >= actual_top_n else []
    
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø N
    vis_dir = os.path.join(output_dir, 'visualizations')
    best_dir = os.path.join(vis_dir, 'best_predictions')
    worst_dir = os.path.join(vis_dir, 'worst_predictions')
    os.makedirs(best_dir, exist_ok=True)
    os.makedirs(worst_dir, exist_ok=True)
    
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø N –ª—É—á—à–∏—Ö
    for i, result in enumerate(top_correct, 1):
        save_path = os.path.join(best_dir, f"{i:02d}_{result['image_name']}_mDice_{result['metrics']['mDice']:.4f}.png")
        visualize_prediction(
            result['image'], result['pred_mask'], result['gt_mask'],
            result['metrics'], result['image_name'], palette, class_names, save_path
        )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø N —Ö—É–¥—à–∏—Ö
    for i, result in enumerate(reversed(top_incorrect), 1):
        save_path = os.path.join(worst_dir, f"{i:02d}_{result['image_name']}_mDice_{result['metrics']['mDice']:.4f}.png")
        visualize_prediction(
            result['image'], result['pred_mask'], result['gt_mask'],
            result['metrics'], result['image_name'], palette, class_names, save_path
        )
    
    logger.info(f"‚úÖ Visualizations saved to {vis_dir}")


def main():
    parser = argparse.ArgumentParser(description='MMSeg test (and eval) a model')
    parser.add_argument('--config', help='train config file path')
    parser.add_argument('--checkpoint', help='checkpoint file')
    parser.add_argument('config_pos', nargs='?', help='train config file path (positional)')
    parser.add_argument('checkpoint_pos', nargs='?', help='checkpoint file (positional)')
    parser.add_argument(
        '--work-dir',
        help=('if specified, the evaluation metric results will be dumped'
              'into the directory as json'))
    parser.add_argument(
        '--data-root', default='datasets/train_dataset_for_students',
        help='Root directory of the dataset')
    parser.add_argument(
        '--split', choices=['train', 'val', 'test'], default='test',
        help='Dataset split to use for testing (default: test)')
    parser.add_argument(
        '--out',
        type=str,
        help='The directory to save output prediction for offline evaluation')
    parser.add_argument(
        '--output-dir',
        type=str,
        help='The directory to save output prediction (alias for --out)')
    parser.add_argument(
        '--save-predictions', action='store_true',
        help='Save prediction results to output directory')
    parser.add_argument(
        '--show', action='store_true', help='show prediction results')
    parser.add_argument(
        '--show-dir',
        help='directory where painted images will be saved. '
        'If specified, it will be automatically saved '
        'to the work_dir/timestamp/show_dir')
    parser.add_argument(
        '--wait-time', type=float, default=2, help='the interval of show (s)')
    parser.add_argument(
        '--cfg-options',
        nargs='+',
        action=DictAction,
        help='override some settings in the used config, the key-value pair '
        'in xxx=yyy format will be merged into config file. If the value to '
        'be overwritten is a list, it should be like key="[a,b]" or key=a,b '
        'It also allows nested list/tuple values, e.g. key="[(a,b),(c,d)]" '
        'Note that the quotation marks are necessary and that no white space '
        'is allowed.')
    parser.add_argument(
        '--launcher',
        choices=['none', 'pytorch', 'slurm', 'mpi'],
        default='none',
        help='job launcher')
    parser.add_argument(
        '--tta', action='store_true', help='Test time augmentation')
    parser.add_argument('--local_rank', '--local-rank', type=int, default=0)
    parser.add_argument('--exp-name', default='test', help='Experiment name')
    parser.add_argument(
        '--visualize-top-n', type=int, default=0,
        help='Visualize top N best and worst predictions (0 = disabled)')

    args = parser.parse_args()
    
    config_path = args.config or args.config_pos
    checkpoint_path = args.checkpoint or args.checkpoint_pos
    
    if not config_path:
        parser.error('--config (or config as positional argument) is required')
    if not checkpoint_path:
        parser.error('--checkpoint (or checkpoint as positional argument) is required')
    
    if 'LOCAL_RANK' not in os.environ:
        os.environ['LOCAL_RANK'] = str(args.local_rank)

    register_all_modules(init_default_scope=True)
    logger.info("="*80)
    logger.info(f"üß™ TESTING: {args.exp_name}")
    logger.info("="*80)
    logger.info(f"‚öôÔ∏è  Config: {config_path}")
    logger.info(f"üì¶ Checkpoint: {checkpoint_path}")
    logger.info(f"üì¶ Data Root: {args.data_root}")
    logger.info(f"üìä Split: {args.split}")
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º output_dir –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    output_dir = args.output_dir or args.out
    if output_dir:
        logger.info(f"üíæ Output Dir: {output_dir}")
    elif args.work_dir:
        logger.info(f"üìÅ Work Dir: {args.work_dir}")
    logger.info("="*80)

    cfg = Config.fromfile(config_path)
    cfg.launcher = args.launcher
    if args.cfg_options is not None:
        cfg.merge_from_dict(args.cfg_options)

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º output_dir –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –∫–∞–∫ work_dir, —á—Ç–æ–±—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å —Ç–∞–º, –∞ –Ω–µ –≤ work_dirs
    metrics_output_dir = args.output_dir or args.out
    
    if metrics_output_dir is None:
        # –ï—Å–ª–∏ output_dir –Ω–µ —É–∫–∞–∑–∞–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º work_dir –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
        if args.work_dir is not None:
            metrics_output_dir = args.work_dir
        elif cfg.get('work_dir', None) is not None:
            metrics_output_dir = cfg.work_dir
        else:
            # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π work_dirs
            metrics_output_dir = os.path.join('./work_dirs',
                                            os.path.splitext(os.path.basename(config_path))[0])
    
    # –í–ê–ñ–ù–û: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º work_dir –î–û —Å–æ–∑–¥–∞–Ω–∏—è Runner, —á—Ç–æ–±—ã –≤—Å–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–≤–∞–ª–∏—Å—å —Ç–∞–º
    # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω --output-dir, —Ç–æ work_dirs –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è –≤–æ–æ–±—â–µ
    cfg.work_dir = metrics_output_dir
    
    logger.info(f"üìÅ Using work_dir: {cfg.work_dir} (temporary files will be created here)")

    cfg.load_from = checkpoint_path

    cfg = setup_test_config(cfg, args.data_root, args.split)

    if args.show or args.show_dir:
        cfg = trigger_visualization_hook(cfg, args)

    if args.tta:
        cfg.test_dataloader.dataset.pipeline = cfg.tta_pipeline
        cfg.tta_model.module = cfg.model
        cfg.model = cfg.tta_model

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º output_dir —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –∑–∞–ø—Ä–æ—à–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    output_dir = None
    if args.save_predictions:
        output_dir = args.output_dir or args.out
        if output_dir is None:
            output_dir = os.path.join(cfg.work_dir, 'test_results')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # test_evaluator –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–≤–∞—Ä–µ–º –∏–ª–∏ —Å–ø–∏—Å–∫–æ–º
        if hasattr(cfg, 'test_evaluator'):
            if isinstance(cfg.test_evaluator, dict):
                cfg.test_evaluator['output_dir'] = output_dir
                cfg.test_evaluator['keep_results'] = True
            elif isinstance(cfg.test_evaluator, list) and len(cfg.test_evaluator) > 0:
                # –ï—Å–ª–∏ —Å–ø–∏—Å–æ–∫, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ (–æ–±—ã—á–Ω–æ —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞)
                cfg.test_evaluator[0]['output_dir'] = output_dir
                cfg.test_evaluator[0]['keep_results'] = True
            logger.info(f"üíæ All predictions will be saved to: {output_dir}")
    else:
        # –ï—Å–ª–∏ –Ω–µ –∑–∞–ø—Ä–æ—à–µ–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π, –æ—Ç–∫–ª—é—á–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ evaluator
        if hasattr(cfg, 'test_evaluator'):
            if isinstance(cfg.test_evaluator, dict):
                cfg.test_evaluator.pop('output_dir', None)
                cfg.test_evaluator['keep_results'] = False
            elif isinstance(cfg.test_evaluator, list):
                # –£–¥–∞–ª—è–µ–º output_dir –∏–∑ –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ –≤ —Å–ø–∏—Å–∫–µ
                for evaluator in cfg.test_evaluator:
                    if isinstance(evaluator, dict):
                        evaluator.pop('output_dir', None)
                        evaluator['keep_results'] = False
            logger.info("üìù Predictions saving disabled (use --save-predictions to enable)")

    # metrics_output_dir —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤—ã—à–µ –∫–∞–∫ cfg.work_dir
    # –ü—Ä–æ—Å—Ç–æ —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    os.makedirs(metrics_output_dir, exist_ok=True)

    runner = Runner.from_cfg(cfg)
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è —Ö—É–∫–∞
    num_classes = 3
    if hasattr(runner.model, 'decode_head') and hasattr(runner.model.decode_head, 'num_classes'):
        num_classes = runner.model.decode_head.num_classes
    
    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ö—É–∫ –¥–ª—è —Å–±–æ—Ä–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ–∫–∞—Ä—Ç–∏–Ω–Ω–æ
    # –≠—Ç–æ –ø–æ–∑–≤–æ–ª–∏—Ç –∏–∑–±–µ–∂–∞—Ç—å –≤—Ç–æ—Ä–æ–≥–æ –ø—Ä–æ—Ö–æ–¥–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
    per_image_metrics_hook = PerImageMetricsHook(num_classes=num_classes)
    runner.register_hook(per_image_metrics_hook, priority='NORMAL')
    
    # –ê–ñ–ù–û: –ò–∑–º–µ—Ä—è–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    # –≠—Ç–æ –≤—Ä–µ–º—è –≤–∫–ª—é—á–∞–µ—Ç: –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥, inference –º–æ–¥–µ–ª–∏,
    # —Ä–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —á–µ—Ä–µ–∑ Evaluator, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ.
    # –≠—Ç–æ "End-to-End Throughput", –∞ –Ω–µ —á–∏—Å—Ç—ã–π "Inference FPS" –º–æ–¥–µ–ª–∏.
    # –î–ª—è —á–∏—Å—Ç–æ–≥–æ FPS –Ω—É–∂–Ω–æ –∏–∑–º–µ—Ä—è—Ç—å —Ç–æ–ª—å–∫–æ –≤—Ä–µ–º—è –≤–Ω—É—Ç—Ä–∏ model.test_step().
    test_start_time = time.time()
    test_results = runner.test()
    test_end_time = time.time()
    test_duration = test_end_time - test_start_time
    
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ per-class –º–µ—Ç—Ä–∏–∫–∏: —Å—É–º–º–∏—Ä—É–µ–º –ø–ª–æ—â–∞–¥–∏ (intersect/union) –ø–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º,
    # –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ —Å—É–º–º–∞—Ä–Ω—ã—Ö –ø–ª–æ—â–∞–¥–µ–π (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥, —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–µ–Ω evaluator)
    per_class_metrics_from_evaluator = {}
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–µ per-class –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ø–æ–∫–∞—Ä—Ç–∏–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if len(per_image_metrics_hook.results) > 0:
        try:
            
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤
            num_classes = per_image_metrics_hook.num_classes
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            total_intersect = np.zeros(num_classes, dtype=np.int64)
            total_union = np.zeros(num_classes, dtype=np.int64)
            total_pred = np.zeros(num_classes, dtype=np.int64)
            total_label = np.zeros(num_classes, dtype=np.int64)
            
            # –°—É–º–º–∏—Ä—É–µ–º –ø–ª–æ—â–∞–¥–∏ –ø–æ –≤—Å–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            for result in per_image_metrics_hook.results:
                pred_mask = result['pred_sem_seg']
                gt_mask = result['gt_sem_seg']
                
                # –í—ã—á–∏—Å–ª—è–µ–º –ø–ª–æ—â–∞–¥–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                for class_idx in range(num_classes):
                    pred_class = (pred_mask == class_idx)
                    gt_class = (gt_mask == class_idx)
                    
                    intersect = np.logical_and(pred_class, gt_class).sum()
                    union = np.logical_or(pred_class, gt_class).sum()
                    pred_area = pred_class.sum()
                    label_area = gt_class.sum()
                    
                    total_intersect[class_idx] += intersect
                    total_union[class_idx] += union
                    total_pred[class_idx] += pred_area
                    total_label[class_idx] += label_area
            
            # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
            for class_idx in range(num_classes):
                with np.errstate(divide='ignore', invalid='ignore'):
                    iou = total_intersect[class_idx] / total_union[class_idx] if total_union[class_idx] > 0 else 0
                    dice = 2 * total_intersect[class_idx] / (total_pred[class_idx] + total_label[class_idx]) if (total_pred[class_idx] + total_label[class_idx]) > 0 else 0
                    acc = total_intersect[class_idx] / total_label[class_idx] if total_label[class_idx] > 0 else 0
                    
                    per_class_metrics_from_evaluator[class_idx] = {
                        'IoU': float(np.round(np.nan_to_num(iou) * 100, 2)),
                        'Dice': float(np.round(np.nan_to_num(dice) * 100, 2)),
                        'Acc': float(np.round(np.nan_to_num(acc) * 100, 2))
                    }
            
        except Exception as e:
            logger.debug(f"Could not compute per-class metrics from per-image results: {e}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º FPS –∏ latency (End-to-End –º–µ—Ç—Ä–∏–∫–∏)
    inference_metrics = {}
    if hasattr(runner, 'test_dataloader') and runner.test_dataloader is not None:
        try:
            total_samples = len(runner.test_dataloader.dataset)
            if total_samples > 0 and test_duration > 0:
                fps = total_samples / test_duration
                latency_ms = (test_duration / total_samples) * 1000  # —Å—Ä–µ–¥–Ω—è—è latency –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                inference_metrics['fps'] = round(fps, 2)
                inference_metrics['latency_ms'] = round(latency_ms, 2)
                inference_metrics['total_time_s'] = round(test_duration, 2)
                inference_metrics['total_samples'] = total_samples
                inference_metrics['note'] = 'End-to-End throughput (includes data loading, preprocessing, inference, and metric calculation)'
        except Exception:
            pass
    
    if (test_results is None or not isinstance(test_results, dict)) and hasattr(runner, 'message_hub'):
        try:
            test_results = {}
            scalars = runner.message_hub.get_scalar('test')
            if scalars:
                for key, buf in scalars.items():
                    try:
                        test_results[key] = buf.current()
                    except Exception:
                        pass
        except Exception:
            pass

    save_metrics_to_json(
        test_results, 
        metrics_output_dir, 
        config_path, 
        checkpoint_path,
        split=args.split,
        runner=runner,
        cfg=cfg,
        inference_metrics=inference_metrics,
        per_class_metrics_from_evaluator=per_class_metrics_from_evaluator if 'per_class_metrics_from_evaluator' in locals() else {}
    )
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ø –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    if args.visualize_top_n > 0:
        visualize_top_predictions(
            runner=runner,
            top_n=args.visualize_top_n,
            split=args.split,
            output_dir=metrics_output_dir,
            class_names=get_class_names(cfg)
        )

    logger.info("="*80)
    logger.info("‚úÖ Testing completed!")
    logger.info("="*80)
    
    # –Ø–≤–Ω—ã–π flush –¥–ª—è Jupyter
    sys.stdout.flush()
    sys.stderr.flush()


if __name__ == '__main__':
    try:
        main()
    finally:
        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –≤ Jupyter
        sys.stdout.flush()
        sys.stderr.flush()
